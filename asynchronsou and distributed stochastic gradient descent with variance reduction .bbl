% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Coates2011An}
A.~Coates, A.~Y. Ng, and H.~Lee, ``An analysis of single-layer networks in
  unsupervised feature learning,'' \emph{Journal of Machine Learning Research},
  vol.~15, pp. 215--223, 2011.

\bibitem{dahl2012context}
G.~E. Dahl, D.~Yu, L.~Deng, and A.~Acero, ``Context-dependent pre-trained deep
  neural networks for large-vocabulary speech recognition,'' \emph{IEEE
  Transactions on Audio, Speech, and Language Processing}, vol.~20, no.~1, pp.
  30--42, 2012.

\bibitem{collobert2008unified}
R.~Collobert and J.~Weston, ``A unified architecture for natural language
  processing: Deep neural networks with multitask learning,'' in \emph{Proc.
  ACM ICML}, 2008, pp. 160--167.

\bibitem{Dean:2012wx}
J.~Dean, G.~Corrado, R.~Monga, K.~C. 0010, M.~Devin, Q.~V. Le, M.~Z. Mao,
  M.~Ranzato, A.~W. Senior, P.~A. Tucker, K.~Yang, and A.~Y. Ng, ``Large scale
  distributed deep networks.'' in \emph{Proc. NIPS}, 2012, pp. 1232--1240.

\bibitem{Li:2014tt}
M.~Li, D.~G. Andersen, J.~W. Park, A.~J. Smola, A.~Ahmed, V.~Josifovski,
  J.~Long, E.~J. Shekita, and B.-Y. Su, ``Scaling distributed machine learning
  with the parameter server,'' in \emph{Proc. UENSIX OSDI}, 2014, pp. 583--598.

\bibitem{Xing:2015ib}
E.~P. Xing, Q.~Ho, W.~Dai, J.~K. Kim, J.~Wei, S.~Lee, X.~Zheng, P.~Xie,
  A.~Kumar, and Y.~Yu, ``Petuum: a new platform for distributed machine
  learning on big data,'' \emph{Transactions on Big Data}, vol.~1, no.~2, pp.
  49--67, 2015.

\bibitem{Johnson:9MAvkbgy}
R.~Johnson and T.~Zhang, ``Accelerating stochastic gradient descent using
  predictive variance reduction,'' \emph{Advances in Neural Information
  Processing Systems}, pp. 315--323, 2013.

\bibitem{Zhao:SZfxEHHg}
S.-Y. Zhao and W.-J. Li, ``Fast asynchronous parallel stochastic gradient
  decent,'' \emph{arXiv:1508.05711}, 2015.

\bibitem{Reddi:2015vj}
S.~J. Reddi, A.~Hefny, S.~Sra, B.~P{\'o}czos, and A.~J. Smola, ``On variance
  reduction in stochastic gradient descent and its asynchronous variants,'' pp.
  2629--2637, 2015.

\bibitem{Cavalcante:2009il}
R.~L. Cavalcante, I.~Yamada, and B.~Mulgrew, ``An adaptive projected
  subgradient approach to learning in diffusion networks,'' \emph{Transactions
  on Signal Processing}, vol.~57, no.~7, pp. 2762--2774, 2009.

\bibitem{Mania:2015wa}
H.~Mania, X.~Pan, D.~Papailiopoulos, B.~Recht, K.~Ramchandran, and M.~I.
  Jordan, ``Perturbed iterate analysis for asynchronous stochastic
  optimization,'' \emph{arXiv:1507.06970}, 2015.

\bibitem{lian2015asynchronous}
X.~Lian, Y.~Huang, Y.~Li, and J.~Liu, ``Asynchronous parallel stochastic
  gradient for nonconvex optimization,'' in \emph{Advances in Neural
  Information Processing Systems}, 2015, pp. 2719--2727.

\bibitem{Yuan:2015ka}
J.~Yuan, F.~Gao, Q.~Ho, W.~Dai, J.~Wei, X.~Zheng, E.~P. Xing, T.-Y. Liu, and
  W.-Y. Ma, ``Lightlda: Big topic models on modest computer clusters,'' in
  \emph{Proc. WWW}, 2015, pp. 1351--1361.

\bibitem{Zhang:2015tp}
S.~Z. Zhang, Ruiliang and J.~T. Kwok, ``Fast distributed asynchronous sgd with
  variance reduction,'' \emph{arXiv:1508.01633}, 2015.

\bibitem{2015_dai_high_performance_ml}
W.~Dai, A.~Kumar, J.~Wei, Q.~Ho, G.~A. Gibson, and E.~P. Xing,
  ``High-performance distributed ml at scale through parameter server
  consistency models,'' in \emph{Proc. AAAI}, 2015, pp. 79--87.

\bibitem{Li:2014uy}
M.~Li, D.~G. Andersen, A.~J. Smola, and K.~Yu, ``Communication efficient
  distributed machine learning with the parameter server,'' \emph{Proc. NIPS},
  pp. 19--27, 2014.

\bibitem{Dai:2013vj}
W.~Dai, J.~Wei, X.~Zheng, J.~K. Kim, S.~Lee, J.~Yin, Q.~Ho, and E.~P. Xing,
  ``Petuum: A framework for iterative-convergent distributed ml,''
  \emph{arXiv:1312.7651}, 2013.

\end{thebibliography}
